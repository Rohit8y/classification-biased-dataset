{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef502419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"Load image files and labels\n",
    "\n",
    "This file contains the method that creates data and labels from a directory.\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def create_data_with_labels(dataset_dir):\n",
    "    \"\"\"Gets numpy data and label array from images that are in the folders\n",
    "    that are in the folder which was given as a parameter. The folders\n",
    "    that are in that folder are identified by the mug they represent and\n",
    "    the folder name starts with the label.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_dir: A string specifying the directory of a dataset\n",
    "    Returns:\n",
    "        data: A numpy array containing the images\n",
    "        labels: A numpy array containing labels corresponding to the images\n",
    "    \"\"\"\n",
    "    image_paths_per_label = collect_paths_to_files(dataset_dir)\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, image_paths in image_paths_per_label.items():\n",
    "        for image_path in image_paths:\n",
    "            img = cv2.imread(str(image_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "    data = np.array([preprocess_image(image.astype(np.float32))\n",
    "                     for image in images])\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def collect_paths_to_files(dataset_dir):\n",
    "    \"\"\"Returns a dict with labels for each subdirectory of the given directory\n",
    "    as keys and lists of the subdirectory's contents as values.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_dir: A string containing the path to a directory containing\n",
    "            subdirectories to different classes.\n",
    "    Returns:\n",
    "        image_paths_per_label: A dict with labels as keys and lists of file\n",
    "        paths as values.\n",
    "    \"\"\"\n",
    "    dataset_dir = Path(dataset_dir)\n",
    "    mug_dirs = [f for f in sorted(os.listdir(dataset_dir)) if not f.startswith('.')]\n",
    "    image_paths_per_label = {\n",
    "        label: [\n",
    "            dataset_dir / mug_dir / '{0}'.format(f)\n",
    "            for f in os.listdir(dataset_dir / mug_dir) if not f.startswith('.')\n",
    "        ]\n",
    "        for label, mug_dir in enumerate(mug_dirs)\n",
    "    }\n",
    "    return image_paths_per_label\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Returns a preprocessed image.\n",
    "\n",
    "    Parameters:\n",
    "        image: A RGB image with pixel values in range [0, 255].\n",
    "    Returns\n",
    "        image: The preprocessed image.\n",
    "    \"\"\"\n",
    "    image = image / 255.\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae37f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"Model to classify mugs\n",
    "\n",
    "This file contains all the model information: the training steps, the batch\n",
    "size and the model itself.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_batch_size():\n",
    "    \"\"\"Returns the batch size that will be used by your solution.\n",
    "    It is recommended to change this value.\n",
    "    \"\"\"\n",
    "    return 1\n",
    "\n",
    "def get_epochs():\n",
    "    \"\"\"Returns number of epochs that will be used by your solution.\n",
    "    It is recommended to change this value.\n",
    "    \"\"\"\n",
    "    return 1\n",
    "\n",
    "def solution(input_layer):\n",
    "    \"\"\"Returns a compiled model.\n",
    "\n",
    "    This function is expected to return a model to identity the different mugs.\n",
    "    The model's outputs are expected to be probabilities for the classes and\n",
    "    and it should be ready for training.\n",
    "    The input layer specifies the shape of the images. The preprocessing\n",
    "    applied to the images is specified in data.py.\n",
    "\n",
    "    Add your solution below.\n",
    "\n",
    "    Parameters:\n",
    "        input_layer: A tf.keras.layers.InputLayer() specifying the shape of the input.\n",
    "            RGB colored images, shape: (width, height, 3)\n",
    "    Returns:\n",
    "        model: A compiled model\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Code of your solution\n",
    "    model = None\n",
    "\n",
    "    # TODO: Return the compiled model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f8224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"Train and evaluate the model\n",
    "\n",
    "This file trains the model upon the training data and evaluates it with\n",
    "the eval data.\n",
    "It uses the arguments it got via the gcloud command.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def train_model(params):\n",
    "    \"\"\"The function gets the training data from the training folder,\n",
    "    the evaluation data from the eval folder and trains your solution\n",
    "    from the model.py file with it.\n",
    "\n",
    "    Parameters:\n",
    "        params: parameters for training the model\n",
    "    \"\"\"\n",
    "    print('inside train')\n",
    "    (train_data, train_labels) = data.create_data_with_labels(\"data/train/\")\n",
    "    (eval_data, eval_labels) = data.create_data_with_labels(\"data/eval/\")\n",
    "\n",
    "    img_shape = train_data.shape[1:]\n",
    "    input_layer = tf.keras.Input(shape=img_shape, name='input_image')\n",
    "\n",
    "    ml_model = model.solution(input_layer)\n",
    "\n",
    "    if ml_model is None:\n",
    "        print(\"No model found. You need to implement one in model.py\")\n",
    "    else:\n",
    "        ml_model.fit(train_data, train_labels,\n",
    "                     batch_size=model.get_batch_size(),\n",
    "                     epochs=model.get_epochs())\n",
    "        ml_model.evaluate(eval_data, eval_labels, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c31d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m tf_logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(tf_logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"The function gets the training data from the training folder,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mthe evaluation data from the eval folder and trains your solution\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mfrom the model.py file with it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    params: parameters for training the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minside train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m (train_data, train_labels) \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_data_with_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/train/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m (eval_data, eval_labels) \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcreate_data_with_labels(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/eval/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m img_shape \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Documents/Projects/ml6CodingChallenge/challenge-find-ml6-mug/trainer/data.py:25\u001b[0m, in \u001b[0;36mcreate_data_with_labels\u001b[0;34m(dataset_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_data_with_labels\u001b[39m(dataset_dir):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124;03m\"\"\"Gets numpy data and label array from images that are in the folders\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    that are in the folder which was given as a parameter. The folders\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    that are in that folder are identified by the mug they represent and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m        labels: A numpy array containing labels corresponding to the images\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     image_paths_per_label \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_paths_to_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/Projects/ml6CodingChallenge/challenge-find-ml6-mug/trainer/data.py:54\u001b[0m, in \u001b[0;36mcollect_paths_to_files\u001b[0;34m(dataset_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"Returns a dict with labels for each subdirectory of the given directory\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mas keys and lists of the subdirectory's contents as values.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    paths as values.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m Path(dataset_dir)\n\u001b[0;32m---> 54\u001b[0m mug_dirs \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     55\u001b[0m image_paths_per_label \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     56\u001b[0m     label: [\n\u001b[1;32m     57\u001b[0m         dataset_dir \u001b[38;5;241m/\u001b[39m mug_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, mug_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mug_dirs)\n\u001b[1;32m     61\u001b[0m }\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_paths_per_label\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args('')\n",
    "tf_logger = logging.getLogger(\"tensorflow\")\n",
    "tf_logger.setLevel(logging.INFO)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(tf_logger.level / 10)\n",
    "train_model(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
